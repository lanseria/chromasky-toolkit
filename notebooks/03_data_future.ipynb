{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c792811",
   "metadata": {},
   "source": [
    "# GFS 预报数据下载 (探索)\n",
    "\n",
    "现在我们来探索如何下载未来的天气预报数据。我们将使用 NOAA 的 GFS (Global Forecast System) 模型，这是一个全球范围、免费公开的预报系统。\n",
    "\n",
    "**核心逻辑**:\n",
    "1.  GFS 每天在 00z, 06z, 12z, 18z (UTC时间) 发布四次预报。\n",
    "2.  我们需要智能地找到当前可用的、最新的那一次预报（称为“运行周期”）。\n",
    "3.  根据我们关心的未来时间点（例如明天的日落），计算出相对于运行周期的“预报时效”（例如，提前24小时预报）。\n",
    "4.  构建下载 URL 并获取 GRIB2 格式的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# --- 1. 从我们的配置文件中导入所有常量 ---\n",
    "# 因为项目是以可编辑模式安装的 (-e .), 我们可以直接从包中导入\n",
    "from chromasky_toolkit import config\n",
    "\n",
    "# --- 3. 设置日志 (这部分也可以保留在 Notebook 中) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"GFS_Downloader\")\n",
    "\n",
    "# --- 4. 验证导入的配置 (可选) ---\n",
    "print(\"\\n--- 使用的配置 ---\")\n",
    "print(f\"Extraction Area (North): {config.AREA_EXTRACTION['north']}\")\n",
    "print(f\"Data Directory: {config.DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GFS 功能所需的额外库\n",
    "import requests\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def _find_latest_available_gfs_run() -> Tuple[str, str] | None:\n",
    "    \"\"\"\n",
    "    智能判断当前可用的最新 GFS 运行周期。\n",
    "    \"\"\"\n",
    "    logger.info(\"--- [GFS] 正在寻找最新的可用运行周期... ---\")\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    safe_margin = timedelta(hours=5)\n",
    "\n",
    "    for i in range(10):\n",
    "        potential_run_time = now_utc - timedelta(hours=i)\n",
    "        run_hour = (potential_run_time.hour // 6) * 6\n",
    "        run_time_utc = potential_run_time.replace(hour=run_hour, minute=0, second=0, microsecond=0)\n",
    "        \n",
    "        if (now_utc - run_time_utc) >= safe_margin:\n",
    "            run_date_str = run_time_utc.strftime('%Y%m%d')\n",
    "            run_hour_str = f\"{run_time_utc.hour:02d}\"\n",
    "            logger.info(f\"✅ 找到最新的可用运行周期: {run_date_str} {run_hour_str}z\")\n",
    "            return run_date_str, run_hour_str\n",
    "            \n",
    "    logger.error(\"❌ 在过去24小时内未能找到任何可用的 GFS 运行周期。\")\n",
    "    return None\n",
    "\n",
    "def _get_future_target_times() -> Dict[str, datetime]:\n",
    "    \"\"\"根据配置，计算未来1-2天我们关心的日出日落事件的 UTC 时间。\"\"\"\n",
    "    local_tz = ZoneInfo(config.LOCAL_TZ)\n",
    "    now_local = datetime.now(local_tz)\n",
    "    today = now_local.date()\n",
    "    tomorrow = today + timedelta(days=1)\n",
    "    future_events = {}\n",
    "    all_times = config.SUNRISE_EVENT_TIMES + config.SUNSET_EVENT_TIMES\n",
    "    for t_str in all_times:\n",
    "        event_time = datetime.strptime(t_str, '%H:%M').time()\n",
    "        today_event_dt = datetime.combine(today, event_time, tzinfo=local_tz)\n",
    "        if today_event_dt > now_local:\n",
    "            future_events[f\"today_{t_str.replace(':', '')}\"] = today_event_dt\n",
    "        tomorrow_event_dt = datetime.combine(tomorrow, event_time, tzinfo=local_tz)\n",
    "        future_events[f\"tomorrow_{t_str.replace(':', '')}\"] = tomorrow_event_dt\n",
    "    return {name: dt.astimezone(timezone.utc) for name, dt in sorted(future_events.items())}\n",
    "\n",
    "def download_gfs_forecast(run_date: str, run_hour: str):\n",
    "    \"\"\"\n",
    "    为给定的 GFS 运行周期，下载所有未来目标事件的数据。\n",
    "    *** 已更新为对每个时间点发起一次合并请求 ***\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- [GFS] 开始为运行周期 {run_date} {run_hour}z 下载数据 ---\")\n",
    "    run_time_utc = datetime.strptime(f\"{run_date}{run_hour}\", \"%Y%m%d%H\").replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    run_dir_name = f\"{run_date}_t{run_hour}z\"\n",
    "    output_dir_base = config.GFS_DATA_DIR / run_dir_name\n",
    "    output_dir_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    manifest_path = output_dir_base / \"manifest.json\"\n",
    "    if manifest_path.exists():\n",
    "        logger.info(f\"✅ 清单文件已存在，跳过此运行周期的下载: {manifest_path}\")\n",
    "        return\n",
    "\n",
    "    target_times_utc = _get_future_target_times()\n",
    "    logger.info(f\"将为以下 {len(target_times_utc)} 个未来事件下载数据...\")\n",
    "\n",
    "    manifest_content = {}\n",
    "    for event_name, target_time in target_times_utc.items():\n",
    "        time_diff_hours = (target_time - run_time_utc).total_seconds() / 3600\n",
    "        forecast_hour = round(time_diff_hours)\n",
    "\n",
    "        logger.info(f\"-> 正在处理事件 '{event_name}' (预报时效: f{forecast_hour:03d})\")\n",
    "        \n",
    "        # --- 关键修改：构建单一的合并请求 ---\n",
    "        dir_param = f\"/gfs.{run_date}/{run_hour}/atmos\"\n",
    "        file_param = f\"gfs.t{run_hour}z.pgrb2.0p25.f{forecast_hour:03d}\"\n",
    "        \n",
    "        params = {\n",
    "            \"file\": file_param,\n",
    "            \"dir\": dir_param,\n",
    "            \"subregion\": \"\",\n",
    "            \"leftlon\": config.AREA_EXTRACTION['west'],\n",
    "            \"rightlon\": config.AREA_EXTRACTION['east'],\n",
    "            \"toplat\": config.AREA_EXTRACTION['north'],\n",
    "            \"bottomlat\": config.AREA_EXTRACTION['south'],\n",
    "        }\n",
    "        \n",
    "        # 将所有需要的变量和层级添加到参数中\n",
    "        for var in config.GFS_VARS:\n",
    "            params[f\"var_{var.upper()}\"] = \"on\"\n",
    "        \n",
    "        req = requests.models.PreparedRequest()\n",
    "        req.prepare_url(config.GFS_BASE_URL, params)\n",
    "        url = req.url\n",
    "        # --- 修正结束 ---\n",
    "\n",
    "        # 文件现在是按事件保存，而不是按数据块\n",
    "        event_dir = output_dir_base / f\"{event_name}_f{forecast_hour:03d}\"\n",
    "        event_dir.mkdir(exist_ok=True)\n",
    "        output_path = event_dir / \"forecast_data.grib2\"\n",
    "        \n",
    "        file_path_in_manifest = None\n",
    "        try:\n",
    "            logger.info(f\"   发起合并下载请求...\")\n",
    "            response = requests.get(url, stream=True, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            logger.info(f\"   ✅ 合并数据已保存到: {output_path}\")\n",
    "            file_path_in_manifest = str(output_path)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"   ❌ 合并下载失败: {e}\")\n",
    "            logger.debug(f\"   - 失败的 URL: {url}\") # 打印URL以供调试\n",
    "        \n",
    "        manifest_content[event_name] = {\n",
    "            \"forecast_hour\": forecast_hour,\n",
    "            \"target_time_utc\": target_time.isoformat(),\n",
    "            \"file_path\": file_path_in_manifest # 清单现在只记录一个文件路径\n",
    "        }\n",
    "\n",
    "    # 写入清单文件\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest_content, f, indent=4, ensure_ascii=False)\n",
    "    logger.info(f\"✅ GFS 数据清单已成功写入: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 执行 GFS 预报数据下载 ---\n",
    "gfs_run_info = _find_latest_available_gfs_run()\n",
    "\n",
    "if gfs_run_info:\n",
    "    run_date_str, run_hour_str = gfs_run_info\n",
    "    download_gfs_forecast(run_date_str, run_hour_str)\n",
    "else:\n",
    "    logger.error(\"未能找到可用的 GFS 运行周期，今日预报下载任务无法执行。\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
