{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfcf071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Config: 未找到 .env 文件于 C:\\Users\\zhang\\Documents\\Code\\chromasky-toolkit\\src\\.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 22:42:13,516 - MapDrawer - INFO - 当前字体不支持中文，开始主动扫描系统可用中文字体...\n",
      "2025-08-07 22:42:13,517 - MapDrawer - INFO - ✅ 找到可用的中文字体: 'PingFang TC'。将其设置为默认字体。\n",
      "2025-08-07 22:42:13,517 - MapDrawer - INFO - 最终使用的字体列表: ['PingFang TC']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 环境设置与导入完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 核心库 ---\n",
    "import cdsapi\n",
    "import logging\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, date\n",
    "from zoneinfo import ZoneInfo\n",
    "from typing import Dict, Set, List, Literal\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# --- 项目模块 ---\n",
    "from chromasky_toolkit import config\n",
    "from chromasky_toolkit.map_drawer import generate_map_from_grid\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# --- 日志设置 ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"PastDataPipeline\")\n",
    "\n",
    "print(\"✅ 环境设置与导入完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d9f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# --- 核心配置区：请在此处设置您的目标参数 ---\n",
    "# ======================================================================\n",
    "\n",
    "# 1. 设置您想分析的本地日期\n",
    "TARGET_LOCAL_DATE_STR = \"2025-05-25\"\n",
    "\n",
    "# 2. 设置您想分析的天文事件 ('sunrise' 或 'sunset')\n",
    "#    这将决定下载哪些小时的数据。\n",
    "EVENT_TYPE: Literal[\"sunrise\", \"sunset\"] = \"sunset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc8ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_utc_times(target_local_date: date, event: Literal[\"sunrise\", \"sunset\"]) -> Dict[str, Set[int]]:\n",
    "    \"\"\"根据目标本地日期和指定的事件，计算出需要下载的UTC日期和小时。\"\"\"\n",
    "    local_tz = ZoneInfo(config.LOCAL_TZ)\n",
    "    # 根据事件类型选择对应的时间列表\n",
    "    event_times_config = config.SUNRISE_EVENT_TIMES if event == \"sunrise\" else config.SUNSET_EVENT_TIMES\n",
    "    \n",
    "    utc_date_hours: Dict[str, Set[int]] = {}\n",
    "    logger.info(f\"为本地日期 {target_local_date} 的 {event} 事件计算所需的 UTC 时间...\")\n",
    "    \n",
    "    for time_str in event_times_config:\n",
    "        local_dt = datetime.combine(target_local_date, datetime.strptime(time_str, '%H:%M').time(), tzinfo=local_tz)\n",
    "        utc_dt = local_dt.astimezone(timezone.utc)\n",
    "        utc_date_str = utc_dt.strftime('%Y-%m-%d')\n",
    "        if utc_date_str not in utc_date_hours:\n",
    "            utc_date_hours[utc_date_str] = set()\n",
    "        utc_date_hours[utc_date_str].add(utc_dt.hour)\n",
    "            \n",
    "    logger.info(f\"计算完成的 UTC 请求信息: {utc_date_hours}\")\n",
    "    return utc_date_hours\n",
    "\n",
    "\n",
    "def download_era5_data_for_event(target_local_date: date, event: Literal[\"sunrise\", \"sunset\"]) -> Path | None:\n",
    "    \"\"\"为指定的本地日期和事件下载 ERA5 再分析数据。\"\"\"\n",
    "    if not config.CDS_API_KEY:\n",
    "        logger.error(\"❌ CDS API 配置未找到，无法继续下载。\")\n",
    "        return None\n",
    "\n",
    "    # 文件名现在包含事件类型，例如 era5_data_sunset.nc\n",
    "    output_dir = config.ERA5_DATA_DIR / target_local_date.strftime('%Y-%m-%d')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    final_output_file = output_dir / f\"era5_data_{event}.nc\"\n",
    "    \n",
    "    if final_output_file.exists():\n",
    "        logger.info(f\"✅ 最终文件 '{final_output_file.name}' 已存在，跳过下载。\")\n",
    "        return final_output_file\n",
    "\n",
    "    required_utc_info = get_required_utc_times(target_local_date, event)\n",
    "    if not required_utc_info:\n",
    "        logger.warning(\"未能计算出任何需要下载的UTC日期和小时。\")\n",
    "        return None\n",
    "\n",
    "    # ... (构建 request_params 的逻辑保持不变)\n",
    "    years, months, days, hours = set(), set(), set(), set()\n",
    "    for utc_date_str, hours_set in required_utc_info.items():\n",
    "        dt_obj = datetime.strptime(utc_date_str, '%Y-%m-%d'); years.add(f\"{dt_obj.year}\"); months.add(f\"{dt_obj.month:02d}\"); days.add(f\"{dt_obj.day:02d}\")\n",
    "        hours.update([f\"{h:02d}:00\" for h in hours_set])\n",
    "    request_params = {'year': sorted(list(years)), 'month': sorted(list(months)), 'day': sorted(list(days)), 'time': sorted(list(hours))}\n",
    "    \n",
    "    logger.info(f\"将为 {event} 事件发起下载请求: {request_params}\")\n",
    "\n",
    "    c = cdsapi.Client(timeout=600, quiet=False, url=\"https://cds.climate.copernicus.eu/api\", key=config.CDS_API_KEY)\n",
    "    temp_download_path = output_dir / f\"temp_download_{event}\"\n",
    "    \n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-single-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis', 'format': 'netcdf',\n",
    "                'variable': [\"high_cloud_cover\", \"medium_cloud_cover\", \"low_cloud_cover\", \"total_cloud_cover\"],\n",
    "                'area': [config.AREA_EXTRACTION[k] for k in [\"north\", \"west\", \"south\", \"east\"]],\n",
    "                **request_params\n",
    "            },\n",
    "            str(temp_download_path)\n",
    "        )\n",
    "        logger.info(f\"✅ 临时文件已成功下载到: {temp_download_path}\")\n",
    "\n",
    "        # ... (处理 ZIP 文件的逻辑保持不变)\n",
    "        if zipfile.is_zipfile(temp_download_path):\n",
    "            with zipfile.ZipFile(temp_download_path, 'r') as z:\n",
    "                nc_file = [f for f in z.namelist() if f.endswith('.nc')][0]\n",
    "                source_path = Path(z.extract(nc_file, path=output_dir))\n",
    "                source_path.rename(final_output_file)\n",
    "        else:\n",
    "            temp_download_path.rename(final_output_file)\n",
    "        \n",
    "        return final_output_file\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 下载或处理时发生错误: {e}\", exc_info=True)\n",
    "        return None\n",
    "    finally:\n",
    "        if temp_download_path.exists(): temp_download_path.unlink()\n",
    "\n",
    "\n",
    "def extract_and_save_all_slices(source_nc_path: Path, variables: List[str]) -> List[Path]:\n",
    "    \"\"\"\n",
    "    从大的 NetCDF 文件中，为所有时间和所有指定变量提取数据切片并保存。\n",
    "    *** 新版本：使用本地时区来命名输出文件。***\n",
    "    \"\"\"\n",
    "    if not source_nc_path.exists():\n",
    "        logger.error(f\"❌ 源文件不存在: {source_nc_path}\")\n",
    "        return []\n",
    "\n",
    "    saved_paths = []\n",
    "    local_tz = ZoneInfo(config.LOCAL_TZ) # 获取本地时区对象\n",
    "\n",
    "    with xr.open_dataset(source_nc_path, engine=\"netcdf4\") as ds:\n",
    "        time_coord_name = 'valid_time' if 'valid_time' in ds.coords else 'time'\n",
    "        time_coord = ds[time_coord_name]\n",
    "        \n",
    "        logger.info(f\"将使用 '{config.LOCAL_TZ}' 时区进行文件名转换。\")\n",
    "        \n",
    "        for utc_dt_numpy in time_coord.values:\n",
    "            # 1. 将 numpy.datetime64 转换为带 UTC 时区的 Python datetime 对象\n",
    "            utc_dt = pd.to_datetime(utc_dt_numpy).to_pydatetime().replace(tzinfo=timezone.utc)\n",
    "            \n",
    "            # 2. 将 UTC 时间转换为本地时间\n",
    "            local_dt = utc_dt.astimezone(local_tz)\n",
    "            \n",
    "            # 3. 使用本地时间构建输出路径和文件名\n",
    "            local_date_str = local_dt.strftime('%Y-%m-%d')\n",
    "            local_time_str_path = local_dt.strftime('%H%M')\n",
    "            \n",
    "            for var in variables:\n",
    "                try:\n",
    "                    # 使用原始的 UTC 时间进行数据切片\n",
    "                    data_slice = ds[var].sel({time_coord_name: utc_dt_numpy}, method='nearest')\n",
    "                    \n",
    "                    # 使用本地时间构建输出目录和文件名\n",
    "                    output_dir = config.PROCESSED_DATA_DIR / local_date_str\n",
    "                    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    output_path = output_dir / f\"{var}_{local_time_str_path}.nc\"\n",
    "                    \n",
    "                    # 添加元数据到文件，记录原始UTC时间\n",
    "                    data_slice.attrs['original_utc_time'] = utc_dt.isoformat()\n",
    "                    \n",
    "                    data_slice.to_netcdf(output_path)\n",
    "                    \n",
    "                    # 日志中同时显示UTC和本地时间，方便追溯\n",
    "                    log_msg = (\n",
    "                        f\"✅ 数据切片 [{var}] \"\n",
    "                        f\"(UTC: {utc_dt.strftime('%H:%M')} -> Local: {local_dt.strftime('%H:%M')}) \"\n",
    "                        f\"已保存到: {output_path.relative_to(config.PROJECT_ROOT)}\"\n",
    "                    )\n",
    "                    logger.info(log_msg)\n",
    "                    saved_paths.append(output_path)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ 提取或保存 {var} @ {utc_dt.strftime('%H:%M')} UTC 时出错: {e}\")\n",
    "    return saved_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3e8a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 22:42:13,547 - PastDataPipeline - INFO - \n",
      "================================================================================\n",
      "2025-08-07 22:42:13,548 - PastDataPipeline - INFO - ===== 开始为 2025-05-25 的 sunset 事件执行数据流水线 =====\n",
      "2025-08-07 22:42:13,549 - PastDataPipeline - INFO - ================================================================================\n",
      "2025-08-07 22:42:13,549 - PastDataPipeline - INFO - \n",
      "--- 步骤 1/2: 下载原始数据 ---\n",
      "2025-08-07 22:42:13,552 - PastDataPipeline - INFO - 为本地日期 2025-05-25 的 sunset 事件计算所需的 UTC 时间...\n",
      "2025-08-07 22:42:13,553 - PastDataPipeline - INFO - 计算完成的 UTC 请求信息: {'2025-05-25': {10, 11, 12, 13}}\n",
      "2025-08-07 22:42:13,554 - PastDataPipeline - INFO - 将为 sunset 事件发起下载请求: {'year': ['2025'], 'month': ['05'], 'day': ['25'], 'time': ['10:00', '11:00', '12:00', '13:00']}\n",
      "2025-08-07 22:42:14,540 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-08-07 22:42:14,540 - ecmwf.datastores.legacy_client - INFO - [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-08-07 22:42:15,520 INFO Request ID is 4f2f6203-3732-4098-b6bb-422cf271b544\n",
      "2025-08-07 22:42:15,520 - ecmwf.datastores.legacy_client - INFO - Request ID is 4f2f6203-3732-4098-b6bb-422cf271b544\n",
      "2025-08-07 22:42:15,873 INFO status has been updated to accepted\n",
      "2025-08-07 22:42:15,873 - ecmwf.datastores.legacy_client - INFO - status has been updated to accepted\n",
      "2025-08-07 22:42:31,875 INFO status has been updated to successful\n",
      "2025-08-07 22:42:31,875 - ecmwf.datastores.legacy_client - INFO - status has been updated to successful\n",
      "2025-08-07 22:42:32,475 - multiurl.base - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-2/2025-08-07/eb572172990d4110436191ed8fe4f2fd.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f373aee50f7404b99290e16097cc545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eb572172990d4110436191ed8fe4f2fd.nc:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 22:42:36,392 - PastDataPipeline - INFO - ✅ 临时文件已成功下载到: C:\\Users\\zhang\\Documents\\Code\\chromasky-toolkit\\src\\data\\raw\\era5\\2025-05-25\\temp_download_sunset\n",
      "2025-08-07 22:42:36,397 - PastDataPipeline - INFO - \n",
      "--- 步骤 2/2: 从 'era5_data_sunset.nc' 提取数据切片 ---\n",
      "2025-08-07 22:42:36,625 - PastDataPipeline - INFO - 将使用 'Asia/Shanghai' 时区进行文件名转换。\n",
      "2025-08-07 22:42:36,657 - PastDataPipeline - INFO - ✅ 数据切片 [hcc] (UTC: 10:00 -> Local: 18:00) 已保存到: data\\processed\\2025-05-25\\hcc_1800.nc\n",
      "2025-08-07 22:42:36,688 - PastDataPipeline - INFO - ✅ 数据切片 [mcc] (UTC: 10:00 -> Local: 18:00) 已保存到: data\\processed\\2025-05-25\\mcc_1800.nc\n",
      "2025-08-07 22:42:36,716 - PastDataPipeline - INFO - ✅ 数据切片 [lcc] (UTC: 10:00 -> Local: 18:00) 已保存到: data\\processed\\2025-05-25\\lcc_1800.nc\n",
      "2025-08-07 22:42:36,740 - PastDataPipeline - INFO - ✅ 数据切片 [hcc] (UTC: 11:00 -> Local: 19:00) 已保存到: data\\processed\\2025-05-25\\hcc_1900.nc\n",
      "2025-08-07 22:42:36,770 - PastDataPipeline - INFO - ✅ 数据切片 [mcc] (UTC: 11:00 -> Local: 19:00) 已保存到: data\\processed\\2025-05-25\\mcc_1900.nc\n",
      "2025-08-07 22:42:36,791 - PastDataPipeline - INFO - ✅ 数据切片 [lcc] (UTC: 11:00 -> Local: 19:00) 已保存到: data\\processed\\2025-05-25\\lcc_1900.nc\n",
      "2025-08-07 22:42:36,817 - PastDataPipeline - INFO - ✅ 数据切片 [hcc] (UTC: 12:00 -> Local: 20:00) 已保存到: data\\processed\\2025-05-25\\hcc_2000.nc\n",
      "2025-08-07 22:42:36,843 - PastDataPipeline - INFO - ✅ 数据切片 [mcc] (UTC: 12:00 -> Local: 20:00) 已保存到: data\\processed\\2025-05-25\\mcc_2000.nc\n",
      "2025-08-07 22:42:36,867 - PastDataPipeline - INFO - ✅ 数据切片 [lcc] (UTC: 12:00 -> Local: 20:00) 已保存到: data\\processed\\2025-05-25\\lcc_2000.nc\n",
      "2025-08-07 22:42:36,890 - PastDataPipeline - INFO - ✅ 数据切片 [hcc] (UTC: 13:00 -> Local: 21:00) 已保存到: data\\processed\\2025-05-25\\hcc_2100.nc\n",
      "2025-08-07 22:42:36,917 - PastDataPipeline - INFO - ✅ 数据切片 [mcc] (UTC: 13:00 -> Local: 21:00) 已保存到: data\\processed\\2025-05-25\\mcc_2100.nc\n",
      "2025-08-07 22:42:36,940 - PastDataPipeline - INFO - ✅ 数据切片 [lcc] (UTC: 13:00 -> Local: 21:00) 已保存到: data\\processed\\2025-05-25\\lcc_2100.nc\n",
      "2025-08-07 22:42:36,942 - PastDataPipeline - INFO - \n",
      "✅ 流水线执行成功！共提取了 12 个数据切片。\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 根据配置解析参数 ---\n",
    "target_date = datetime.strptime(TARGET_LOCAL_DATE_STR, \"%Y-%m-%d\").date()\n",
    "variables_to_process = [\"hcc\", \"mcc\", \"lcc\"] \n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(f\"===== 开始为 {target_date} 的 {EVENT_TYPE} 事件执行数据流水线 =====\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "\n",
    "# --- 2. 步骤一: 下载原始数据 ---\n",
    "logger.info(\"\\n--- 步骤 1/2: 下载原始数据 ---\")\n",
    "raw_data_path = download_era5_data_for_event(target_date, EVENT_TYPE)\n",
    "\n",
    "\n",
    "# --- 3. 步骤二: 提取所有数据切片 ---\n",
    "if raw_data_path and raw_data_path.exists():\n",
    "    logger.info(f\"\\n--- 步骤 2/2: 从 '{raw_data_path.name}' 提取数据切片 ---\")\n",
    "    processed_files = extract_and_save_all_slices(raw_data_path, variables_to_process)\n",
    "    \n",
    "    if processed_files:\n",
    "        logger.info(f\"\\n✅ 流水线执行成功！共提取了 {len(processed_files)} 个数据切片。\")\n",
    "    else:\n",
    "        logger.error(\"\\n😢 流水线失败：未能提取任何数据切片。\")\n",
    "else:\n",
    "    logger.error(\"\\n😢 流水线失败：原始数据下载失败，无法继续。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
